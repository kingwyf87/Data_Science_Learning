{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Model Evaluation Error Metrics\n",
    "1. Confusion Matrix\n",
    "2. Gain and Lift Chart\n",
    "3. Kolmogorov Smirnov Chart\n",
    "4. AUC – ROC\n",
    "5. Gini Coefficient\n",
    "6. Concordant – Discordant Ratio\n",
    "7. Root Mean Squared Error\n",
    "8. Cross Validation\n",
    "\n",
    "The evaluation metrics used regression model (continuous output) or a classification model (nominal or binary output) are different.\n",
    "\n",
    "In classification problems, we use two types of algorithms (dependent on the kind of output it creates):\n",
    "\n",
    "1. Class output: Algorithms like SVM and KNN create a class output. For instance, in a binary classification problem, the outputs will be either 0 or 1. However, today we have algorithms which can convert these class outputs to probability. But these algorithms are not well accepted by the statistics community.\n",
    "2. Probability output: Algorithms like Logistic Regression, Random Forest, Gradient Boosting, Adaboost etc. give probability outputs. Converting probability outputs to class output is just a matter of creating a threshold probability.\n",
    "\n",
    "In regression problems, we do not have such inconsistencies in output. The output is always continuous in nature and requires no further treatment.\n",
    "\n",
    "## 1. Confusion Matrix\n",
    "\n",
    "A confusion matrix is an $N \\times N$ matrix, where $N$ is the number of classes being predicted.\n",
    "\n",
    "## 2. Gain and Lift charts\n",
    "\n",
    "Gain and Lift chart are mainly concerned to check the rank ordering of the probabilities. Here are the steps to build a Lift/Gain chart:\n",
    "\n",
    "Step 1 : Calculate probability for each observation\n",
    "\n",
    "Step 2 : Rank these probabilities in decreasing order.\n",
    "\n",
    "Step 3 : Build deciles with each group having almost 10% of the observations.\n",
    "\n",
    "Step 4 : Calculate the response rate at each deciles for Good (Responders), Bad (Non-responders) and total.\n",
    "\n",
    "http://www2.cs.uregina.ca/~dbd/cs831/notes/lift_chart/lift_chart.html\n",
    "\n",
    "## 3. Kolomogorov Smirnov chart\n",
    "\n",
    "K-S or Kolmogorov-Smirnov chart measures performance of classification models. More accurately, K-S is a measure of the degree of separation between the positive and negative distributions. The K-S is 100, if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives.\n",
    "\n",
    "## 4. Area Under the ROC curve (AUC – ROC)\n",
    "\n",
    "The biggest advantage of using ROC curve is that it is independent of the change in proportion of responders.\n",
    "\n",
    "The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity) is also known as false positive rate and sensitivity is also known as True Positive rate. Following is the ROC curve for the case in hand.\n",
    "\n",
    "Lift is dependent on total response rate of the population. Hence, if the response rate of the population changes, the same model will give a different lift chart. A solution to this concern can be true lift chart (finding the ratio of lift and perfect model lift at each decile). But such ratio rarely makes sense for the business.\n",
    "\n",
    "ROC curve on the other hand is almost independent of the response rate. This is because it has the two axis coming out from columnar calculations of confusion matrix. The numerator and denominator of both x and y axis will change on similar scale in case of response rate shift.\n",
    "\n",
    "## 5. Gini Coefficient\n",
    "\n",
    "Gini coefficient is sometimes used in classification problems. Gini coefficient can be straigh away derived from the AUC ROC number. Gini is nothing but ratio between area between the ROC curve and the diagnol line & the area of the above triangle. Following is the formulae used :\n",
    "\n",
    "Gini = 2*AUC – 1\n",
    "\n",
    "## 6. Concordant – Discordant ratio\n",
    "\n",
    "This is again one of the most important metric for any classification predictions problem.\n",
    "\n",
    "## 7. Root Mean Squared Error (RMSE)\n",
    "\n",
    "RMSE is the most popular evaluation metric used in regression problems. It follows an assumption that error are unbiased and follow a normal distribution. Here are the key points to consider on RMSE:\n",
    "\n",
    "1. The power of ‘square root’  empowers this metric to show large number deviations.\n",
    "2. The ‘squared’ nature of this metric helps to deliver more robust results which prevents cancelling the positive and negative error values. In other words, this metric aptly displays the plausible magnitude of error term.\n",
    "3. It avoids the use of absolute error values which is highly undesirable in mathematical calculations.\n",
    "4. When we have more samples, reconstructing the error distribution using RMSE is considered to be more reliable.\n",
    "5. RMSE is highly affected by outlier values. Hence, make sure you’ve removed outliers from your data set prior to using this metric.\n",
    "6. As compared to mean absolute error, RMSE gives higher weightage and punishes large errors.\n",
    "\n",
    "## 8. Cross Validation\n",
    "\n",
    "Cross Validation is one of the most important concepts in any type of data modelling. It simply says, try to leave a sample on which you do not train the model and test the model on this sample before finalizing the model.\n",
    "\n",
    "k-fold cross validation is widely used to check whether a model is an overfit or not. If the performance metrics at each of the k times modelling are close to each other and the mean of metric is highest. \n",
    "\n",
    "Generally a value of k = 10 is recommended for most purpose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
